{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "def load_data(train_directory, test_file):\n",
    "    train = pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "    train_texts, train_labels = [], []\n",
    "    print(\"Loading : train data\")\n",
    "    for file in os.listdir(train_directory+\"/neg\"):\n",
    "        with open(train_directory+\"/neg/\"+file, \"r\") as f:\n",
    "            train_texts.append(f.read())\n",
    "            train_labels.append(-1)\n",
    "    for file in os.listdir(train_directory +\"/pos\"):\n",
    "        with open(train_directory +\"/pos/\" + file, \"r\") as f:\n",
    "            train_texts.append(f.read())\n",
    "            train_labels.append(1)\n",
    "    test = pd.DataFrame(columns=[\"text\"])\n",
    "    test_texts = []\n",
    "    print(\"Loading : test data\")\n",
    "    with open(test_file, \"r\") as f:\n",
    "        for line in f : \n",
    "            line = line.rstrip()\n",
    "            test_texts.append(line)\n",
    "    train[\"text\"] = train_texts\n",
    "    train[\"label\"] = train_labels\n",
    "    test[\"text\"] = test_texts\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    train[\"label\"] = encoder.fit_transform(train[\"label\"])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading : train data\n",
      "Loading : test data\n"
     ]
    }
   ],
   "source": [
    "train, test= load_data(\"../data/movies1000\", \"../data/moviesTest/testSentiment.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text brut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "count_vect = CountVectorizer(analyzer='word',lowercase = False ,ngram_range= (1,3), min_df=5,max_df=0.5)\n",
    "count_vect.fit(train[\"text\"])\n",
    "train_count =  count_vect.transform(train[\"text\"])\n",
    "Scores = cross_val_score( clf, train_count, train[\"label\"], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86  , 0.8325, 0.855 , 0.8325, 0.8775])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = pd.DataFrame()\n",
    "stemmed[\"label\"] = train[\"label\"]\n",
    "stemmed[\"text\"] = train[\"text\"].apply(lambda x : \" \".join([stemmer.stem(i) for i in word_tokenize(x)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed.to_csv (\"../cache/stemmed.csv\", index = False, header=True) # saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# hide axes\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "the_table = plt.table(cellText=best_words,\n",
    "                      colLabels=[\"Mitterrand\", \"Chirac\"],\n",
    "                      loc='bottom')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
